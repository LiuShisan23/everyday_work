# 2024/3/16

今天阅读一些polyu computing系的教授，今天要把陶瓷教授的list做完。

今天还重温了一下transformer，阅读了这篇文章https://jalammar.github.io/illustrated-transformer/，里面关于tensor2tensor的部分是tensorflow写的所以还没有看，准备也学习一下。

现在的项目因为涉及到peft和lora以及transformer，所以我要先了解一下bert和peft，然后跑一下bert和lora的结合项目，不知道自己的显卡能不能跑，不能跑就组一下平台吧，顺便可以了解一下如何分布式运行这个模型，为项目做一下积累。

因为需要了解如保存和读取模型，所以把这部前面的unet网络的搭建再温习一遍。

好好好，unet里面涉及到up-sampling中的up-conv所以把up-sampling学习一一遍。https://juejin.cn/post/7229965046079897661

关于nn.upsample https://www.cnblogs.com/wanghui-garcia/p/11399053.html 这里面的mode参数涉及到双线性插值算法，我也不会，菜鸡继续学https://zhuanlan.zhihu.com/p/110754637，https://www.cnblogs.com/wancy/p/15212604.html (双线性插值的数学推导)

nn.torch.各种模块：https://pytorch.org/docs/stable/nn.html#pooling-layers

Unet：https://zhuanlan.zhihu.com/p/313283141

bert：https://zhuanlan.zhihu.com/p/98855346

transformer的这篇文章很牛，是我见过讲的最清楚的了https://jalammar.github.io/illustrated-transformer/

什么是token以及cls

什么是预训练pretraining

