# 2024/3/20

继续完成昨天没有完成的部分。

不能急于求成！一定要一步一个脚印扎扎实实地来完成你的目标！

关于decode函数，是可以将word_id反转为对应的word或者sub_word

```python

wnut_large_tokenized = wnut_large.map(
    tokenize_and_align_labels, fn_kwargs={"llm_tok": tokenizer}, batched=True,
    remove_columns=wnut["train"].column_names
)
```

关于上面的map()函数

finetuning分为很多种吧，一种是上游一种是下游。所以downstream和upstream的微调都有哪些应用实例呢？

用trainer API训练模型：https://huggingface.co/learn/nlp-course/zh-CN/chapter3/3

如何用dataset去load_datasets:https://huggingface.co/docs/datasets/loading

dictionary的items()函数： https://www.runoob.com/python/att-dictionary-items.html

关于tqdm：https://zhuanlan.zhihu.com/p/163613814

关于huggingface的pipeline: https://huggingface.co/docs/transformers/main/zh/main_classes/pipelines

lora部分的细节：https://zhuanlan.zhihu.com/p/658007966

microsoft的lora部分：主要是调用loralib，应该更peft还不一样。

Target modules for applying PEFT / LoRA on different models：https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models?newreg=f2f2180c35bc4afbb5555a802475ab72

现在先把lora+bert解决，然后仅对roberta的encoder或者decoder或者隔层进行lora



现在要解决的问题是：对于lora后的得到的lora_A和lora_B矩阵，如何获取。针对下述的模型进行分析，有十二个bert_layer，

~~~python
BeftModelForTokenClassification(
  (base_model): LoraModel(
    (model): BertForTokenClassification(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0-11): 12 x BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (key): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): ModulesToSaveWrapper(
        (original_module): Linear(in_features=768, out_features=13, bias=True)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=768, out_features=13, bias=True)
        )
      )
    )
  )
)
~~~

咋改代码，有点难度。

保存lora_A weight 并应用给下一层。

把源码一定要好好解读，这样才知道怎么改以及如何finetuning！

项目bert-lora-tensorrt对应blog：https://martin-dittgen.de/blogpost/lora-from-scatch.html







